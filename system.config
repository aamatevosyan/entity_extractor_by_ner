### use # to comment out the configure item

################ Status ################
mode=train
# string: train/interactive_predict

################ Datasets(Input/Output) ################
datasets_fold=data/news
train_file=train.txt
dev_file=dev.txt

delimiter=b
# string: (t: "\t";"table")|(b: "backspace";" ")|(other, e.g., '|||', ...)

# Finetune-Bert+Crf: use_bert=True, use_bilstm=False, finetune=True
# Finetune-Bert+BiLstm+Crf: use_bert=True, use_bilstm=True, finetune=True
# Bert+BiLstm+Crf: use_bert=True, use_bilstm=True, finetune=False
# BiLstm+Crf: use_bert=False, use_bilstm=True, finetune=False

use_bert=True

use_bilstm=True

finetune=False

vocabs_dir=data/news/vocabs

log_dir=data/news/logs

checkpoints_dir=checkpoints/bilstm-crf-news

checkpoint_name=model

################ Labeling Scheme ################
label_scheme=BIESO
# string: BIO/BIESO

label_level=2
# int, 1:BIO/BIESO; 2:BIO/BIESO + suffix
# max to 2

hyphen=-
# string: -|_, for connecting the prefix and suffix: `B_PER', `I_LOC'

suffix=[ORGANIZATION,PERSON,DATE]
# unnecessary if label_level=1

measuring_metrics=[precision,recall,f1,accuracy]
# string: accuracy|precision|recall|f1
# f1 is compulsory

################ Model Configuration ################
embedding_dim=768
# int

hidden_dim=768

max_sequence_length=200
# int, cautions! set as a LARGE number as possible,
# this will be kept during training and inferring, text having length larger than this will be truncated.

CUDA_VISIBLE_DEVICES=0
# int, -1:CPU, [0,]:GPU
# coincides with tf.CUDA_VISIBLE_DEVICES

seed=42

################ Training Settings ################
epoch=1
batch_size=16

dropout=0.5
learning_rate=0.01

optimizer=Adam
# string: SGD/Adagrad/AdaDelta/RMSprop/Adam

checkpoints_max_to_keep=3
print_per_batch=20

is_early_stop=True
patient=5
# unnecessary if is_early_stop=False

#Model
bert_model_path=DeepPavlov/rubert-base-cased
from_pt=True

